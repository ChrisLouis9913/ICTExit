{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuNqUclsToaQlUKX3XWhfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisLouis9913/ICTExit/blob/main/FinalTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import joblib\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geH7gPXNzFLD",
        "outputId": "c236be1b-b8aa-4b1b-be00-94f038597c83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    # remove HTML tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    # keep only letters and numbers\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    return [t for t in nltk.word_tokenize(text) if t not in STOPWORDS]"
      ],
      "metadata": {
        "id": "XuhyoFgNzUdD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Reviews.csv\")  # path to the Kaggle dataset file\n",
        "print(df.shape)\n",
        "df = df[['Text', 'Score']].dropna().copy()\n",
        "# Map Score to binary label: 4-5 -> Positive (1), 1-2 -> Negative (0), drop 3\n",
        "df = df[df['Score'] != 3]\n",
        "df['label'] = (df['Score'] >= 4).astype(int)\n",
        "df['clean_text'] = df['Text'].apply(clean_text)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
        "print(\"Examples:\", df.shape)\n",
        "X = df['clean_text'].values\n",
        "y = df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajkznaPzhZz",
        "outputId": "2f53f04a-d337-434f-e965-e10f37b486e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6263, 10)\n",
            "Examples: (5784, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Baseline Model (TF-IDF + Logistic Regression)\n",
        "Workflow: Implement a classical NLP pipeline. This involves text cleaning (e.g., lowercasing, removing stop words), vectorizing the text using TF-IDF, and training a Logistic Regression model on the vectorized data. Analytical Question: In a markdown cell, explain why TF-IDF is often a better choice for text classification than a simple Bag of Words (Count Vectorizer)."
      ],
      "metadata": {
        "id": "FL5j1FEm4n7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga')\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = lr.predict(X_test_tfidf)\n",
        "y_prob_lr = lr.predict_proba(X_test_tfidf)[:,1]\n",
        "\n",
        "print(\"TF-IDF + Logistic Regression\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"F1:\", f1_score(y_test, y_pred_lr))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ImiD8KXzxQz",
        "outputId": "5a3fc559-5765-4ab6-ecee-a8cd8624f594"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF + Logistic Regression\n",
            "Accuracy: 0.8928262748487468\n",
            "F1: 0.9345300950369588\n",
            "ROC-AUC: 0.9349349349349348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF reduces the influence of very common words across the corpus by down-weighting terms that occur frequently in many documents, while up-weighting terms that are more specific to particular documents. This helps classifiers focus on discriminative words rather than those that are common but not informative. A simple Bag of Words (count vectorizer) treats all words equally, so frequent but uninformative tokens may dominate, and the model can overfit to high-frequency tokens that don't carry class information. TF-IDF also normalizes by document length implicitly (depending on implementation), which helps when reviews vary in length."
      ],
      "metadata": {
        "id": "BBC8wiMI0dtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Word Embedding Model (Word2Vec + Random Forest)\n",
        "Workflow: Train a Word2Vec model on your corpus of review text to learn custom word embeddings. Create a feature vector for each review by averaging the Word2Vec vectors of the words within it. Train a Random Forest classifier on these averaged feature vectors. Analytical Question: In a markdown cell, what is one key advantage of using Word2Vec embeddings over TF-IDF for capturing the meaning of a text?"
      ],
      "metadata": {
        "id": "1LiFc0oW42ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare tokenized lists for Word2Vec\n",
        "sentences_train = [simple_tokenize(text) for text in X_train]\n",
        "# Train Word2Vec on training sentences\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences_train,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    epochs=10,\n",
        "    seed=42\n",
        ")\n",
        "w2v_model.save(\"w2v_model.kv\")  # save\n",
        "\n",
        "# Function to average word vectors for a document\n",
        "def doc_vector_avg(tokens, model, vector_size=100):\n",
        "    vecs = []\n",
        "    for t in tokens:\n",
        "        if t in model.wv:\n",
        "            vecs.append(model.wv[t])\n",
        "    if len(vecs) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "# Build averaged vectors\n",
        "X_train_w2v = np.vstack([doc_vector_avg(simple_tokenize(t), w2v_model) for t in X_train])\n",
        "X_test_w2v = np.vstack([doc_vector_avg(simple_tokenize(t), w2v_model) for t in X_test])\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "rf.fit(X_train_w2v, y_train)\n",
        "y_pred_rf = rf.predict(X_test_w2v)\n",
        "y_prob_rf = rf.predict_proba(X_test_w2v)[:,1]\n",
        "\n",
        "print(\"Word2Vec (avg) + RandomForest\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"F1:\", f1_score(y_test, y_pred_rf))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "LXxdBg_f1tbz",
        "outputId": "d67b8995-9291-4725-8f06-5ea0ef9cb179"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-775284577.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare tokenized lists for Word2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train Word2Vec on training sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m w2v_model = Word2Vec(\n\u001b[1;32m      5\u001b[0m     \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3607812122.py\u001b[0m in \u001b[0;36msimple_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimple_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec captures semantic relationships between words: words with similar contexts have similar vectors. This allows the model to generalize across synonyms and related expressions, capturing semantic similarity that TF-IDF (a sparse, frequency-based representation) does not. TF-IDF treats each token as independent and cannot capture similarity between different tokens."
      ],
      "metadata": {
        "id": "RydtixLD2U2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Deep Learning Model (RNN/LSTM)\n",
        "Workflow: Implement a simple sequential model using a Recurrent Neural Network (RNN) or an LSTM layer for the same sentiment classification task. This will require tokenizing the text and padding sequences. Analytical Question: In a markdown cell, why is an LSTM often preferred over a simple RNN for text classification tasks? (Hint: Mention the vanishing gradient problem)."
      ],
      "metadata": {
        "id": "mm9hRm335CC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "MAX_WORDS = 20000\n",
        "MAX_LEN = 200\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "# Build a simple LSTM\n",
        "tf.random.set_seed(42)\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN),\n",
        "    Bidirectional(LSTM(128, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "mc = ModelCheckpoint('lstm_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=6,\n",
        "    batch_size=256,\n",
        "    callbacks=[es, mc]\n",
        ")\n",
        "\n",
        "y_prob_lstm = model.predict(X_test_pad, batch_size=512).ravel()\n",
        "y_pred_lstm = (y_prob_lstm >= 0.5).astype(int)\n",
        "print(\"LSTM\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
        "print(\"F1:\", f1_score(y_test, y_pred_lstm))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "P608hwUX3Ks0",
        "outputId": "9d26a05e-80bc-4b77-e9e7-3607d56ac1e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7443 - loss: 0.5880"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.7521 - loss: 0.5797 - val_accuracy: 0.8510 - val_loss: 0.4261\n",
            "Epoch 2/6\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8258 - loss: 0.4676"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8265 - loss: 0.4658 - val_accuracy: 0.8510 - val_loss: 0.3694\n",
            "Epoch 3/6\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8319 - loss: 0.3955"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8332 - loss: 0.3925 - val_accuracy: 0.8683 - val_loss: 0.3064\n",
            "Epoch 4/6\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9085 - loss: 0.2369 - val_accuracy: 0.8683 - val_loss: 0.3343\n",
            "Epoch 5/6\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9546 - loss: 0.1345 - val_accuracy: 0.8942 - val_loss: 0.4180\n",
            "Epoch 6/6\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9738 - loss: 0.0859 - val_accuracy: 0.8898 - val_loss: 0.4270\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "LSTM\n",
            "Accuracy: 0.8366464995678479\n",
            "F1: 0.9052631578947369\n",
            "ROC-AUC: 0.834128573017462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple RNNs suffer from the vanishing (and exploding) gradient problems when learning long-range dependencies across time steps. Gradients propagated through many time steps can shrink exponentially, preventing the network from learning dependencies that span long distances. LSTM (Long Short-Term Memory) networks use gated mechanisms (input, forget, and output gates) that control the flow of information and gradients, allowing them to preserve and propagate relevant information over longer sequences and mitigate the vanishing gradient issue. Thus, LSTMs are better at capturing longer context in text."
      ],
      "metadata": {
        "id": "DQ1geaPV3T1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Comparative Analysis and Recommendation\n",
        "Workflow: Thoroughly evaluate all three of your models (TF-IDF, Word2Vec, RNN/LSTM) on the test set using Accuracy, F1-Score, and ROC-AUC. Create a comparison table in your notebook summarizing the performance of the three models. Analytical Question: Based on your results, which model would you recommend for deployment? Justify your choice by considering not just the performance metrics but also the trade-offs in model complexity, training time, and interpretability."
      ],
      "metadata": {
        "id": "Tsr-ZGf26Uf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    \"tfidf_lr\": {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred_lr),\n",
        "        \"f1\": f1_score(y_test, y_pred_lr),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_prob_lr)\n",
        "    },\n",
        "    \"w2v_rf\": {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred_rf),\n",
        "        \"f1\": f1_score(y_test, y_pred_rf),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_prob_rf)\n",
        "    },\n",
        "    \"lstm\": {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred_lstm),\n",
        "        \"f1\": f1_score(y_test, y_pred_lstm),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_prob_lstm)\n",
        "    }\n",
        "}\n",
        "pd.DataFrame(results).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "collapsed": true,
        "id": "40Vd9R7z3g39",
        "outputId": "bfeec9b4-a96a-4415-9e53-e17f109993bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_pred_rf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3730767208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     },\n\u001b[1;32m      7\u001b[0m     \"w2v_rf\": {\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"roc_auc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred_rf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Example answer template)\n",
        "- If LSTM gives best F1/ROC-AUC by a clear margin: Recommend deploying LSTM if the production environment can support GPU or sufficient CPU inference latency is acceptable. LSTM offers highest performance for sequences and can capture context, but training time and inference latency are higher and interpretability is low.\n",
        "- If TF-IDF + Logistic has competitive performance: Recommend Logistic Regression for deployment because it is fast to train and infer, easy to scale, and highly interpretable (coefficients map to words). This is valuable for business insights.\n",
        "- If Word2Vec + RandomForest performs best: It can strike a balance between capturing semantics and being more interpretable than deep models (though not as interpretable as Logistic due to averaged embeddings). RandomForest inference and model size may be heavier but still manageable.\n",
        "\n",
        "Considerations:\n",
        "- Complexity and cost: LSTM requires more compute for training and often for inference.\n",
        "- Interpretability: Logistic Regression wins.\n",
        "- Training time: TF-IDF + Logistic < Word2Vec+RF < LSTM.\n",
        "- If performance gains of the complex model are marginal, prefer the simpler model (Logistic) for production."
      ],
      "metadata": {
        "id": "LUtPWvKN3l9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Deployment with Streamlit\n",
        "Workflow: Save your recommended model and any necessary vectorizers or tokenizers. Build a Streamlit application where a user can type or paste a new product review into a text area. The app must use your saved model to predict the sentiment (\"Positive\" or \"Negative\") and display the result clearly to the user."
      ],
      "metadata": {
        "id": "FPiwfxaO7J0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save artifacts\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.joblib\")\n",
        "joblib.dump(lr, \"logistic_model.joblib\")\n",
        "joblib.dump(rf, \"rf_w2v.joblib\")\n",
        "w2v_model.save(\"w2v_model.kv\")\n",
        "joblib.dump(tokenizer, \"tokenizer.joblib\")\n",
        "model.save(\"lstm_model.h5\")\n",
        "\n",
        "# Save results/metadata (choose best by F1)\n",
        "best_name = max(results.keys(), key=lambda k: results[k]['f1'])\n",
        "meta = {\n",
        "    \"best_model\": best_name,\n",
        "    \"results\": results,\n",
        "    \"files\": {\n",
        "        \"tfidf_vectorizer\": \"tfidf_vectorizer.joblib\",\n",
        "        \"logistic\": \"logistic_model.joblib\",\n",
        "        \"rf_w2v\": \"rf_w2v.joblib\",\n",
        "        \"w2v_model\": \"w2v_model.kv\",\n",
        "        \"tokenizer\": \"tokenizer.joblib\",\n",
        "        \"lstm\": \"lstm_model.h5\"\n",
        "    }\n",
        "}\n",
        "with open(\"best_model_info.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(\"Best model:\", best_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "collapsed": true,
        "id": "5hHARs1M4Nlk",
        "outputId": "8dbae80b-99f1-4dab-e95b-7f8816235b4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-130588190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tfidf_vectorizer.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"logistic_model.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rf_w2v.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w2v_model.kv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tokenizer.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) BONUS TASK : Model Interpretation\n",
        "Workflow: For your best-performing classical model (either Logistic Regression or Random Forest), implement a feature to show why it made a certain prediction. For Logistic Regression, you can extract the top 10 words with the highest TF-IDF scores that contributed most to the \"Positive\" and \"Negative\" predictions. Analytical Question: In a markdown cell, explain how providing this \"word importance\" feature can help the marketing team gain actionable insights beyond a simple sentiment score."
      ],
      "metadata": {
        "id": "hnrVyOqR7lqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_logistic_prediction(text, tfidf, lr, top_k=10):\n",
        "    ct = clean_text(text)\n",
        "    x = tfidf.transform([ct])\n",
        "    feature_names = np.array(tfidf.get_feature_names_out())\n",
        "    coefs = lr.coef_[0]  # coef for positive class\n",
        "    # contributions = tfidf_value * coef\n",
        "    contributions = x.toarray()[0] * coefs\n",
        "    top_pos_idx = np.argsort(contributions)[-top_k:][::-1]\n",
        "    top_neg_idx = np.argsort(contributions)[:top_k]\n",
        "    top_pos = list(zip(feature_names[top_pos_idx], contributions[top_pos_idx]))\n",
        "    top_neg = list(zip(feature_names[top_neg_idx], contributions[top_neg_idx]))\n",
        "    return top_pos, top_neg\n",
        "\n",
        "example_text = \"This product was excellent and arrived quickly, works as expected.\"\n",
        "top_pos, top_neg = explain_logistic_prediction(example_text, tfidf, lr)\n",
        "print(\"Top positive contributors:\", top_pos)\n",
        "print(\"Top negative contributors:\", top_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178Jd8ZE4SlN",
        "outputId": "c6979fe9-3d39-41d8-eb74-fa65a8da3e2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top positive contributors: [('excellent', np.float64(0.32047867439298794)), ('works', np.float64(0.20987430794116185)), ('quickly', np.float64(0.1368939746467566)), ('and', np.float64(0.11926780396562568)), ('arrived', np.float64(0.08937940784907325)), ('as', np.float64(0.07314760659899573)), ('excellent and', np.float64(0.06261109022993493)), ('arrived quickly', np.float64(0.057285909541440284)), ('was excellent', np.float64(0.03336595285591319)), ('and arrived', np.float64(0.025016617479117226))]\n",
            "Top negative contributors: [('was', np.float64(-0.2559954773522665)), ('product was', np.float64(-0.10616172464473478)), ('product', np.float64(-0.06130739003241431)), ('as expected', np.float64(-0.05141370802906511)), ('expected', np.float64(-0.04077330378155722)), ('this', np.float64(-0.024639273277574328)), ('works as', np.float64(-0.005436921195390629)), ('this product', np.float64(-0.003915895317916437)), ('reddish', np.float64(-0.0)), ('reduces', np.float64(0.0))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Providing word-level contributions (word importance) turns a single numeric sentiment score into actionable, interpretable reasons. Marketing can see which specific terms drive positive or negative sentiment (e.g., \"arrived quickly\", \"easy to use\", \"broken\", \"poor quality\"). This helps:\n",
        "- Identify recurring product issues (words associated with negative predictions).\n",
        "- Surface product features or service aspects that customers praise.\n",
        "- Inform targeted copywriting or FAQ updates by emphasizing terms customers value.\n",
        "- Prioritize fixes by frequency + impact: words that occur often and contribute strongly to negative sentiment are high priority.\n",
        "- Measure changes over time: track whether certain negative keywords become less important after UX or product changes.\n",
        "Interpretability fosters trust and enables data-driven actions rather than opaque model outputs."
      ],
      "metadata": {
        "id": "CZZrPBS84WcT"
      }
    }
  ]
}